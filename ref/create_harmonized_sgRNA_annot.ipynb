{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create harmonized sgRNA guide annotation file for use with the CRISPR pipeline (2025)\n",
    "This notebook describes the creation of a unified annotation file from the guide annotation files provided by the Hon, Huangfu, and Gersbach labs, according to the specification described in: https://github.com/pinellolab/CRISPR_Pipeline/blob/main/example_data/guide_metadata.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install matplotlib\n",
    "#%pip install numpy\n",
    "#%pip install seaborn\n",
    "#%pip install biomart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths: TODO update if necessary\n",
    "#local_path = \"/cellar/users/aklie/data/datasets/tf_perturb_seq/ref/\"\n",
    "#local_path = \"C:/Users/seg95/Documents/tf_perturb_seq/\"\n",
    "local_path = \"/hpc/group/gersbachlab/seg95/tf_perturb_seq/ref/\"\n",
    "#local_path = \"D:/tf_perturb_seq/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import merged guide reference file, along with guide index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged guide ref file\n",
    "merged_guide_file = pd.read_csv(local_path + \"outer_merged_file.csv\")\n",
    "print(merged_guide_file.head())\n",
    "\n",
    "merged_guide_file_poolabcd = pd.read_csv(local_path + \"outer_merged_file_poolabcd.csv\")\n",
    "merged_guide_file_poolf = pd.read_csv(local_path + \"outer_merged_file_poolf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgRNA index files\n",
    "sgrna_index_poolabcd = pd.read_csv(local_path + \"sgRNA_index_v0.csv\", sep = \"\\t\")\n",
    "sgrna_index_poolf = pd.read_csv(local_path + \"igvf_poolF_annotation.csv\", sep = \"\\t\")\n",
    "\n",
    "sgrna_index_dacc_annot = pd.read_csv(local_path + \"sgRNA_index_dacc_annot_reference.csv\", sep = \"\\t\")\n",
    "print(len(set(sgrna_index_dacc_annot['protospacer']).intersection(set(merged_guide_file_poolabcd['protospacer']))))\n",
    "\n",
    "def adjust_index_file(sgrna_index, name_sgrna_seq = 'sgRNA_seq', add_leading_G = True):\n",
    "    if(name_sgrna_seq == \"sgRNA_seq\"):\n",
    "        sgrna_index['strand'] = sgrna_index['target_loc'].str.extract(r'\\((\\+|\\-)\\)')\n",
    "        sgrna_index['oligo'] = sgrna_index['oligo'].str.upper()\n",
    "    else:\n",
    "        sgrna_index['oligo_sequence'] = sgrna_index['oligo_sequence'].str.upper()\n",
    "    sgrna_index[name_sgrna_seq] = sgrna_index[name_sgrna_seq].str.upper()\n",
    "    # Adjust the index file to add leading Gs if needed\n",
    "    if(add_leading_G):\n",
    "        sgrna_index[name_sgrna_seq] = 'G' + sgrna_index[name_sgrna_seq]\n",
    "    return sgrna_index\n",
    "\n",
    "sgrna_index_poolabcd = adjust_index_file(sgrna_index_poolabcd)\n",
    "sgrna_index_poolf = adjust_index_file(sgrna_index_poolf, name_sgrna_seq= 'protospacer', add_leading_G = False)\n",
    "\n",
    "sgrna_index_dacc_annot['protospacer'] = sgrna_index_dacc_annot['protospacer'].str.upper()\n",
    "sgrna_index_poolf['protospacer'] = sgrna_index_poolf['protospacer'].str.upper()\n",
    "#sgrna_index_dacc_annot['protospacer'] = [s[1:] if len(s) > 0 else s for s in sgrna_index_dacc_annot['protospacer']]\n",
    "#sgrna_index_dacc_annot['reverse_compliment'] = sgrna_index_dacc_annot['reverse_compliment'].str.rstrip('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a reverse compliment if needed\n",
    "def reverse_compliment(sequence):\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
    "    return \"\".join(complement.get(base, base) for base in reversed(sequence.upper()))\n",
    "\n",
    "sgrna_index_poolabcd['reverse_compliment'] = sgrna_index_poolabcd['sgRNA_seq'].apply(reverse_compliment)\n",
    "sgrna_index_poolf.rename(columns={\"antisense_sequence\": \"reverse_compliment\"})\n",
    "\n",
    "print(\"Index:\")\n",
    "print(sgrna_index_poolabcd.head())\n",
    "print(sgrna_index_poolf.head())\n",
    "print(\"Annot:\")\n",
    "print(sgrna_index_dacc_annot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_index_dacc_annot[\"protospacer_upper\"] = sgrna_index_dacc_annot[\"protospacer\"].str.upper() \n",
    "\n",
    "print(len(set(sgrna_index_poolabcd['sgRNA_seq']).intersection(sgrna_index_dacc_annot['protospacer_upper'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading 'G' from the DACC annot file\n",
    "def harmonize_leading_G(df_left, df_right, left_col, right_col, debug=True):\n",
    "    \"\"\"\n",
    "    Compare sequence lengths between two DataFrames.\n",
    "    If all (non‑NaN) sequences in one DataFrame start with 'G' and are one base\n",
    "    longer than the other, remove the leading 'G' to harmonize lengths.\n",
    "    If both DataFrames contain a 'reverse_compliment' column, remove one trailing\n",
    "    'C' from that column as well when trimming Gs.\n",
    "\n",
    "    Returns (left_fixed, right_fixed)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    left = df_left.copy()\n",
    "    right = df_right.copy()\n",
    "\n",
    "    # Normalize sequence text\n",
    "    left[left_col] = left[left_col].astype(str).str.strip().str.upper()\n",
    "    right[right_col] = right[right_col].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Compute basic stats\n",
    "    left_lens = left[left_col].dropna().str.len()\n",
    "    right_lens = right[right_col].dropna().str.len()\n",
    "    avg_left, avg_right = left_lens.mean(), right_lens.mean()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Average seq length: left={avg_left:.1f}, right={avg_right:.1f}\")\n",
    "        print(\"Value_counts of left lengths:\", left_lens.value_counts().head().to_dict())\n",
    "        print(\"Value_counts of right lengths:\", right_lens.value_counts().head().to_dict())\n",
    "\n",
    "    # Decide which side to trim \n",
    "    trimmed = None\n",
    "    if np.nanmedian(left_lens) == np.nanmedian(right_lens) + 1:\n",
    "        starts_with_G = left[left_col].dropna().str.startswith(\"G\").all()\n",
    "        if starts_with_G:\n",
    "            left[left_col] = left[left_col].str.replace(r\"^G\", \"\", regex=True)\n",
    "            trimmed = \"left\"\n",
    "            if debug:\n",
    "                print(f\"Removed leading 'G' from all non‑NaN sequences in '{left_col}'.\")\n",
    "        else:\n",
    "            if debug:\n",
    "                mism = left.loc[~left[left_col].dropna().str.startswith(\"G\"), left_col].head(10).tolist()\n",
    "                print(f\"Not all left sequences start with 'G'. Examples: {mism}\")\n",
    "\n",
    "    elif np.nanmedian(right_lens) == np.nanmedian(left_lens) + 1:\n",
    "        starts_with_G = right[right_col].dropna().str.startswith(\"G\").all()\n",
    "        if starts_with_G:\n",
    "            right[right_col] = right[right_col].str.replace(r\"^G\", \"\", regex=True)\n",
    "            trimmed = \"right\"\n",
    "            if debug:\n",
    "                print(f\"Removed leading 'G' from all non‑NaN sequences in '{right_col}'.\")\n",
    "        else:\n",
    "            if debug:\n",
    "                mism = right.loc[~right[right_col].dropna().str.startswith(\"G\"), right_col].head(10).tolist()\n",
    "                print(f\"Not all right sequences start with 'G'. Examples: {mism}\")\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"No consistent 19 / 20 bp offset found; no trimming performed.\")\n",
    "\n",
    "    # --- If both have reverse_compliment, trim trailing C accordingly ---\n",
    "    if \"reverse_compliment\" in left.columns and \"reverse_compliment\" in right.columns:\n",
    "        if trimmed == \"left\":\n",
    "            left[\"reverse_compliment\"] = left[\"reverse_compliment\"].astype(str).str.replace(r\"C$\", \"\", regex=True)\n",
    "            if debug:\n",
    "                print(\"Trimmed trailing 'C' from left.reverse_compliment.\")\n",
    "        elif trimmed == \"right\":\n",
    "            right[\"reverse_compliment\"] = right[\"reverse_compliment\"].astype(str).str.replace(r\"C$\", \"\", regex=True)\n",
    "            if debug:\n",
    "                print(\"Trimmed trailing 'C' from right.reverse_compliment.\")\n",
    "\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_index_dacc_annot, sgrna_index_poolabcd = harmonize_leading_G(\n",
    "    df_left=sgrna_index_dacc_annot,\n",
    "    df_right=sgrna_index_poolabcd,\n",
    "    left_col=\"protospacer_upper\",\n",
    "    right_col=\"sgRNA_seq\",\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the 18 with 21 bp\n",
    "extra_long = sgrna_index_poolabcd[sgrna_index_poolabcd[\"sgRNA_seq\"].str.len() == 21]\n",
    "print(len(extra_long))\n",
    "print(extra_long[\"sgRNA_seq\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the leading 'G' from those 18\n",
    "mask = sgrna_index_poolabcd[\"sgRNA_seq\"].str.len() > 20\n",
    "sgrna_index_poolabcd.loc[mask, \"sgRNA_seq\"] = (\n",
    "    sgrna_index_poolabcd.loc[mask, \"sgRNA_seq\"].str[1:]\n",
    ")\n",
    "print(sgrna_index_poolabcd[\"sgRNA_seq\"].str.len().value_counts().head())\n",
    "\n",
    "if \"reverse_compliment\" in sgrna_index_poolabcd.columns:\n",
    "    sgrna_index_poolabcd.loc[mask, \"reverse_compliment\"] = (\n",
    "        sgrna_index_poolabcd.loc[mask, \"reverse_compliment\"].astype(str).str[:-1]\n",
    "    )\n",
    "    print(\"Also removed trailing base from reverse_compliment for those rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge pool A-D index and DACC files into one; pool F file has sufficient info for matching\n",
    "sgrna_index_merged = pd.merge(\n",
    "    sgrna_index_dacc_annot,\n",
    "    sgrna_index_poolabcd,\n",
    "    left_on=['protospacer_upper', 'reverse_compliment'],\n",
    "    right_on=['sgRNA_seq', 'reverse_compliment'],\n",
    "    how=\"outer\"\n",
    ")\n",
    "print(sgrna_index_merged.head())\n",
    "print(sgrna_index_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out positive/ negative controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_controls = pd.read_csv(local_path + \"negative_controls.tsv\", sep = \"\\t\")\n",
    "pos_controls = pd.read_csv(local_path + \"positive_controls.tsv\", sep = \"\\t\")\n",
    "non_targeting = pd.read_csv(local_path + \"non_targeting.tsv\", sep = \"\\t\")\n",
    "\n",
    "print(non_targeting.head())\n",
    "print(pos_controls.head())\n",
    "print(neg_controls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(sgrna_index_merged['protospacer_upper']).intersection(set(non_targeting['Photospacer (same for all 3 sets)']))))\n",
    "print(len(set(sgrna_index_merged['protospacer_upper']).intersection(set(pos_controls['Photospacer (represent 10 times)']))))  \n",
    "cols = [c for c in neg_controls.columns if c.startswith('Photospacer')]\n",
    "neg_spacers = pd.concat([neg_controls[c] for c in cols]).dropna().astype(str)\n",
    "\n",
    "len(set(sgrna_index_merged['protospacer_upper']).intersection(set(neg_spacers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional file to add missing coordinates\n",
    "poolD_coords = pd.read_csv(local_path + \"pool_D_controls.csv\", sep = \"\\t\")\n",
    "print(poolD_coords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgrna_index_merged, poolD_coords = harmonize_leading_G(\n",
    "    df_left=sgrna_index_merged,\n",
    "    df_right=poolD_coords,\n",
    "    left_col=\"protospacer_upper\",\n",
    "    right_col=\"spacer\",\n",
    "    debug=True\n",
    ")\n",
    "print(sgrna_index_merged[\"protospacer_upper\"].str.len().value_counts().head())\n",
    "print(poolD_coords[\"spacer\"].str.len().value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_poolD_matches(sgrna_df, poolD_df):\n",
    "    sgrna_df = sgrna_df.copy()\n",
    "    poolD_df = poolD_df.copy()\n",
    "\n",
    "    # Normalize spacers\n",
    "    sgrna_df[\"spacer_norm\"] = sgrna_df[\"protospacer_upper\"].astype(str).str.strip().str.upper()\n",
    "    poolD_df[\"spacer_norm\"] = poolD_df[\"spacer\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Define \"broken\" or placeholder coordinates\n",
    "    placeholder_vals = [\"chrPC\", \"chrPC:0-0\", \"0\", \"chrNA\", \"NA\", \"nan\", \"\"]\n",
    "\n",
    "    broken_mask = (\n",
    "        sgrna_df[\"chr_target\"].astype(str).isin(placeholder_vals)\n",
    "        | sgrna_df[\"chr_start_target\"].astype(str).isin(placeholder_vals)\n",
    "        | sgrna_df[\"chr_end_target\"].astype(str).isin(placeholder_vals)\n",
    "    )\n",
    "    broken_rows = sgrna_df[broken_mask]\n",
    "\n",
    "    print(f\"Total rows with placeholder coordinates: {broken_rows.shape[0]}\")\n",
    "\n",
    "    # How many of these have a matching Pool D spacer?\n",
    "    matched = broken_rows[\"spacer_norm\"].isin(poolD_df[\"spacer_norm\"])\n",
    "    print(f\"Of those, {matched.sum()} have a matching spacer in Pool D\")\n",
    "\n",
    "    # Inspect a few examples of matched vs unmatched\n",
    "    print(\"\\nExample matched spacers:\")\n",
    "    print(broken_rows.loc[matched, [\"protospacer_upper\", \"chr_target\", \"protospacer_ID\", \"target_loc\"]].head())\n",
    "    print(broken_rows.loc[matched, [\"protospacer_ID\"]])\n",
    "\n",
    "    print(\"\\nExample unmatched spacers:\")\n",
    "    print(broken_rows.loc[~matched, [\"protospacer_upper\", \"chr_target\", \"protospacer_ID\", \"target_loc\"]].head())\n",
    "\n",
    "    return broken_rows.loc[matched]\n",
    "\n",
    "broken_with_matches = debug_poolD_matches(sgrna_index_merged, poolD_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_from_poolD(sgrna_df, poolD_df):\n",
    "    sgrna_df = sgrna_df.copy()\n",
    "    poolD_df = poolD_df.copy()\n",
    "\n",
    "    # Normalize spacers\n",
    "    sgrna_df[\"spacer_norm\"] = sgrna_df[\"protospacer_upper\"].astype(str).str.strip().str.upper()\n",
    "    poolD_df[\"spacer_norm\"] = poolD_df[\"spacer\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Columns to fill and their mapping from Pool D\n",
    "    column_mapping = {\n",
    "        \"guide_chr\"             : \"chr_target\",\n",
    "        \"guide_start\"           : \"chr_start_target\",\n",
    "        \"guide_end\"             : \"chr_end_target\",\n",
    "        \"strand_pd\"             : \"strand\",\n",
    "        \"intended_target_chr\"   : \"chr_element\",\n",
    "        \"intended_target_start\" : \"chr_start_element\",\n",
    "        \"intended_target_end\"   : \"chr_end_element\",\n",
    "    }\n",
    "\n",
    "    # Keep only relevant columns from Pool D\n",
    "    pool_lookup = poolD_df[[\"spacer_norm\"] + list(column_mapping.keys())]\n",
    "\n",
    "    # Merge Pool D into sgrna_df\n",
    "    merged = pd.merge(sgrna_df, pool_lookup, on=\"spacer_norm\", how=\"left\", suffixes=('', '_poolD'))\n",
    "\n",
    "    # Identify rows with placeholders / missing data\n",
    "    invalid_vals = [\"chrPC\", \"chrPC:0-0\", \"0\", \"chrNA\", \"NA\", \"nan\", \"\"]\n",
    "    broken_mask = (\n",
    "        merged[\"chr_target\"].isna() |\n",
    "        merged[\"chr_target\"].astype(str).isin(invalid_vals) |\n",
    "        merged[\"chr_start_target\"].isna() |\n",
    "        merged[\"chr_start_target\"].astype(str).isin(invalid_vals) |\n",
    "        merged[\"chr_end_target\"].isna() |\n",
    "        merged[\"chr_end_target\"].astype(str).isin(invalid_vals) \n",
    "    )\n",
    "\n",
    "    total_broken = broken_mask.sum()\n",
    "    print(f\"Total broken rows: {total_broken}\")\n",
    "\n",
    "    # Rows that have a matching Pool D spacer\n",
    "    poolD_matches = broken_mask & merged[\"guide_chr\"].notna()\n",
    "    print(f\"Broken rows with Pool D match: {poolD_matches.sum()}\")\n",
    "\n",
    "    # Fill all columns at once using mapping\n",
    "    for pool_col, sgrna_col in column_mapping.items():\n",
    "        if pool_col in merged.columns and sgrna_col in merged.columns:\n",
    "            merged.loc[poolD_matches, sgrna_col] = merged.loc[poolD_matches, pool_col]\n",
    "\n",
    "    merged[\"target_loc\"] = merged[\"chr_target\"].combine_first(pd.Series(invalid_vals)) + \":\" + \\\n",
    "                       merged[\"chr_start_target\"].astype(str) + \"-\" + merged[\"chr_end_target\"].astype(str)\n",
    "\n",
    "    merged[\"element_seq\"] = merged[\"chr_element\"].combine_first(pd.Series(invalid_vals)) + \":\" + \\\n",
    "                        merged[\"chr_start_element\"].astype(str) + \"-\" + merged[\"chr_end_element\"].astype(str)\n",
    "\n",
    "    # Drop temporary columns\n",
    "    merged.drop(columns=[\"spacer_norm\"] + list(column_mapping.keys()), inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "invalid_vals = [\"chrPC\", \"chrPC:0-0\", \"0\", \"chrNA\", \"NA\", \"nan\", \"\"]\n",
    "missing_before = sgrna_index_merged[\n",
    "    sgrna_index_merged[\"chr_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_target\"].astype(str).isin(invalid_vals) |\n",
    "    sgrna_index_merged[\"chr_start_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_start_target\"].astype(str).isin(invalid_vals) |\n",
    "    sgrna_index_merged[\"chr_end_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_end_target\"].astype(str).isin(invalid_vals)\n",
    "]\n",
    "print(\"Rows missing before:\", missing_before.shape[0])\n",
    "broken_indices = set(missing_before.index)\n",
    "\n",
    "poolD_coords = poolD_coords.rename(columns={\"strand\": \"strand_pd\"})\n",
    "sgrna_index_merged = fill_from_poolD(sgrna_index_merged, poolD_coords)\n",
    "\n",
    "missing_after = sgrna_index_merged[\n",
    "    sgrna_index_merged[\"chr_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_target\"].astype(str).isin(invalid_vals) |\n",
    "    sgrna_index_merged[\"chr_start_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_start_target\"].astype(str).isin(invalid_vals) |\n",
    "    sgrna_index_merged[\"chr_end_target\"].isna() |\n",
    "    sgrna_index_merged[\"chr_end_target\"].astype(str).isin(invalid_vals)\n",
    "]\n",
    "\n",
    "print(\"Rows missing after:\", missing_after.shape[0])\n",
    "fixed_indices = list(broken_indices - set(missing_after.index))\n",
    "print(f\"Rows newly filled ({len(fixed_indices)}):\")\n",
    "print(sgrna_index_merged.loc[fixed_indices])\n",
    "\n",
    "sgrna_index_merged.to_csv(local_path + \"sgRNA_index_v0_dacc_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat to resemble input to the CRISPR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import example file for the CRISPR pipeline\n",
    "example_crispr_file = pd.read_csv(local_path + \"crispr_annot_sample.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_crispr_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only necessary columns and reorder them to match \n",
    "def prune_and_rename_cols(merged_guide_file, is_pool_f=False):\n",
    "    # start from full table so you don't drop rows prematurely\n",
    "    df = merged_guide_file.copy()\n",
    "\n",
    "    if is_pool_f:\n",
    "        df[\"guide_id\"] = (\n",
    "            df.get(\"id_gersbach\").combine_first(df.get(\"id_engreitz\"))\n",
    "        )\n",
    "        df[\"intended_target_name\"] = (\n",
    "            df.get(\"intended_target_name_gersbach\")\n",
    "            .combine_first(df.get(\"intended_target_name_engreitz\"))\n",
    "        )\n",
    "    else:\n",
    "        df[\"guide_id\"] = (\n",
    "            df.get(\"id_hon\")\n",
    "            .combine_first(df.get(\"id_gersbach\"))\n",
    "            .combine_first(df.get(\"id_engreitz\"))\n",
    "            .combine_first(df.get(\"id_huangfu\"))\n",
    "        )\n",
    "        df[\"intended_target_name\"] = (\n",
    "            df.get(\"intended_target_name_hon\")\n",
    "            .combine_first(df.get(\"intended_target_name_gersbach\"))\n",
    "            .combine_first(df.get(\"intended_target_name_engreitz\"))\n",
    "            .combine_first(df.get(\"intended_target_name_huangfu\"))\n",
    "        )\n",
    "\n",
    "    # Fallbacks for control / non‑targeting rows.\n",
    "    #df[\"guide_id\"] = df[\"guide_id\"].fillna(df.get(\"id\", df.get(\"protospacer\")))\n",
    "    \n",
    "    # if 'intended_target_name' is NA, set explicitly to np.nan; otherwise fill from 'type' if available\n",
    "    df[\"intended_target_name\"] = np.where(\n",
    "        df[\"intended_target_name\"].isna(),\n",
    "        df.get(\"type\", np.nan),\n",
    "        df[\"intended_target_name\"]\n",
    "    )\n",
    "\n",
    "    # Rename after all adjustments\n",
    "    if \"protospacer\" in df.columns:\n",
    "        df = df.rename(columns={\"protospacer\": \"spacer\"})\n",
    "\n",
    "    keep_cols = [c for c in [\"guide_id\", \"spacer\", \"type\", \"intended_target_name\", \"reverse_compliment\"] if c in df.columns]\n",
    "    ref_clean_sub = df[keep_cols].copy()\n",
    "\n",
    "    print(f\"Retained rows: {ref_clean_sub.shape[0]}\")\n",
    "    return ref_clean_sub\n",
    "\n",
    "# Call function\n",
    "ref_clean_sub = prune_and_rename_cols(merged_guide_file)\n",
    "ref_clean_sub_poolabcd = prune_and_rename_cols(merged_guide_file_poolabcd)\n",
    "ref_clean_sub_poolf = prune_and_rename_cols(merged_guide_file_poolf, is_pool_f=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading 'G' if all spacers have one\n",
    "def strip_leading_G(df, column=\"spacer\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize to uppercase strings for checking\n",
    "    seqs = df[column].astype(str).str.strip().str.upper()\n",
    "    non_empty = seqs[seqs != \"\"]\n",
    "\n",
    "    # Split by length\n",
    "    lens = non_empty.str.len()\n",
    "    long_seqs = non_empty[lens >= 20]\n",
    "    short_seqs = non_empty[lens < 20]\n",
    "\n",
    "    print(f\"Found {len(long_seqs)} sequences with ≥20 nt and {len(short_seqs)} sequences with <20.\")\n",
    "\n",
    "    if long_seqs.empty:\n",
    "        print(\"No sequences ≥20 nt — nothing to test.\")\n",
    "        return df\n",
    "    # For 20 bp sequences, check if they all start with G\n",
    "    starts_with_G = long_seqs.str.startswith(\"G\").all()\n",
    "\n",
    "    if starts_with_G:\n",
    "        df[column] = seqs.str.replace(r\"^G\", \"\", regex=True)\n",
    "        print(f\"All ≥20‑nt sequences start with 'G' - removed leading 'G' from every {column}.\")\n",
    "    else:\n",
    "        print(\"Not all ≥20‑nt sequences start with 'G' - leaving data unchanged.\")\n",
    "        non_g = long_seqs[~long_seqs.str.startswith(\"G\")]\n",
    "        if not non_g.empty:\n",
    "            show_n = min(10, len(non_g))\n",
    "            print(f\"{len(non_g)} long sequences lack leading 'G'; examples:\")\n",
    "            for s in non_g.head(show_n):\n",
    "                print(\" \", s)\n",
    "\n",
    "    return df\n",
    "\n",
    "ref_clean_sub = strip_leading_G(ref_clean_sub)\n",
    "ref_clean_sub_poolabcd = strip_leading_G(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = strip_leading_G(ref_clean_sub_poolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'type' column to targeting/ non-targeting (optional), and then add a 'label' column containing information about positive/ negative controls\n",
    "def simplify_type_column(df):\n",
    "    df = df.copy()\n",
    "    # Copy original type values into 'label'\n",
    "    df[\"label\"] = df[\"type\"]\n",
    "    # Simplify 'type' values, if needed\n",
    "    #df[\"type\"] = np.where(df[\"type\"] == \"non_targeting\", \"non_targeting\", \"targeting\")\n",
    "    return df\n",
    "    \n",
    "ref_clean_sub = simplify_type_column(ref_clean_sub)\n",
    "ref_clean_sub_poolabcd = simplify_type_column(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = simplify_type_column(ref_clean_sub_poolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'targeting' column; if type == targeting, set to True, otherwise False\n",
    "def check_targeting(value):\n",
    "    if(value == \"targeting\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def add_targeting_col(ref_clean_sub):\n",
    "    ref_clean_sub['targeting'] = ref_clean_sub['type'].apply(check_targeting)\n",
    "    order = ['guide_id', 'spacer', 'targeting', 'type', 'intended_target_name', 'label']\n",
    "    ref_clean_sub = ref_clean_sub[order]\n",
    "    print(ref_clean_sub.head())\n",
    "    return ref_clean_sub\n",
    "\n",
    "ref_clean_sub = add_targeting_col(ref_clean_sub)\n",
    "ref_clean_sub_poolabcd = add_targeting_col(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = add_targeting_col(ref_clean_sub_poolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PAM\n",
    "def add_pam(ref_clean_sub):\n",
    "    ref_clean_sub['pam'] = np.nan\n",
    "     # Assign 'NGG' only to targeting rows \n",
    "    ref_clean_sub.loc[ref_clean_sub[\"type\"] == \"targeting\", \"pam\"] = \"NGG\"\n",
    "    print(ref_clean_sub.head())\n",
    "    print(ref_clean_sub.shape)\n",
    "    return ref_clean_sub\n",
    "\n",
    "ref_clean_sub = add_pam(ref_clean_sub)\n",
    "ref_clean_sub_poolabcd = add_pam(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = add_pam(ref_clean_sub_poolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add genomic element column (promoters for everything by non-targeting)\n",
    "def add_genomic_element(ref_clean_sub):\n",
    "    ref_clean_sub['genomic_element'] = pd.Series(\n",
    "        ['promoter' if x != 'non_targeting' else pd.NA for x in ref_clean_sub['label']],\n",
    "        dtype=\"object\"\n",
    "    )\n",
    "    return ref_clean_sub\n",
    "\n",
    "ref_clean_sub = add_genomic_element(ref_clean_sub)\n",
    "ref_clean_sub_poolabcd = add_genomic_element(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = add_genomic_element(ref_clean_sub_poolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for repeated spacer sequences in index file\n",
    "print(sgrna_index_merged['protospacer_upper'].value_counts().loc[lambda x: x > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple mappings from sgrna_index_merged\n",
    "def deduplicate_index_file(df):\n",
    "    def chrom_rank(chrom):\n",
    "        if pd.isna(chrom):\n",
    "            return 100\n",
    "        if isinstance(chrom, str) and chrom.startswith(\"chr\"):\n",
    "            c = chrom[3:]\n",
    "            if c.isdigit():\n",
    "                return int(c)\n",
    "            elif c == \"X\":\n",
    "                return 23\n",
    "            elif c == \"Y\":\n",
    "                return 24\n",
    "        return 100  # fallback\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Rank and sorting\n",
    "    df[\"chrom_rank\"] = df[\"chr_target\"].map(chrom_rank)\n",
    "    df[\"sort_key\"] = (\n",
    "        df[\"chrom_rank\"].fillna(100) * 1e12 +\n",
    "        df[\"chr_start_target\"].fillna(0) * 1e6 +\n",
    "        (df[\"chr_end_target\"].fillna(0) - df[\"chr_start_target\"].fillna(0))\n",
    "    )\n",
    "    \n",
    "    # Group by spacer sequence\n",
    "    grouped = df.groupby(\"protospacer_upper\", group_keys=False)\n",
    "    \n",
    "    # Keep only groups where all key columns are the same across rows\n",
    "    key_cols = [\n",
    "        \"chr_target\", \"chr_start_target\", \"chr_end_target\",\n",
    "        \"chr_element\", \"chr_start_element\", \"chr_end_element\"\n",
    "    ]\n",
    "    \n",
    "    def is_consistent(group):\n",
    "        return all(group[col].nunique(dropna=False) == 1 for col in key_cols)\n",
    "    \n",
    "    consistent_df = grouped.filter(is_consistent)\n",
    "    \n",
    "    # Deduplicate remaining consistent rows by keeping best ranked\n",
    "    deduped_df = (\n",
    "        consistent_df.sort_values(\"sort_key\")\n",
    "                     .drop_duplicates(subset=\"protospacer_upper\", keep=\"first\")\n",
    "                     .drop(columns=[\"chrom_rank\", \"sort_key\"])\n",
    "    )\n",
    "    \n",
    "    return deduped_df\n",
    "\n",
    "# Apply deduplication before merging\n",
    "sgrna_index_merged = deduplicate_index_file(sgrna_index_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'guide_chr', 'guide_start', and 'guide_end' values, which are given as 'chr_target', 'chr_start_target', 'chr_end_target', and 'strand'\n",
    "def add_guide_coords(ref_clean_sub, sgrna_index_merged):\n",
    "    ref_clean_sub = pd.merge(\n",
    "        ref_clean_sub,\n",
    "        sgrna_index_merged[['protospacer_upper', 'chr_target', 'chr_start_target', 'chr_end_target', 'strand']],\n",
    "        left_on='spacer',\n",
    "        right_on='protospacer_upper',\n",
    "        how='left'\n",
    "    )\n",
    "    # Remove protospacer_upper column\n",
    "    ref_clean_sub = ref_clean_sub.drop(columns=['protospacer_upper'])\n",
    "    # Rename intended guide names\n",
    "    ref_clean_sub.rename(columns={'chr_target': 'guide_chr', \n",
    "                                  'chr_start_target': 'guide_start',\n",
    "                                  'chr_end_target': 'guide_end'},\n",
    "                                  inplace=True)\n",
    "\n",
    "\n",
    "    #print(ref_clean_sub.head())\n",
    "    return ref_clean_sub\n",
    "\n",
    "\n",
    "ref_clean_sub_poolabcd = add_guide_coords(ref_clean_sub_poolabcd, sgrna_index_merged)\n",
    "print(ref_clean_sub_poolabcd.head())\n",
    "#print(sgrna_index_merged[sgrna_index_merged['protospacer_upper'] == 'CGGCGACCCTAGGAGAGGT'])\n",
    "#print(ref_clean_sub_poolabcd[ref_clean_sub_poolabcd['spacer'] == 'CGGCGACCCTAGGAGAGGT'])\n",
    "\n",
    "# Columns are already correctly labeled for pool F\n",
    "ref_clean_sub_poolf = pd.merge(\n",
    "    ref_clean_sub_poolf,\n",
    "    sgrna_index_poolf[['protospacer', 'guide_chr', 'guide_start', 'guide_end', 'strand']],\n",
    "    left_on='spacer',\n",
    "    right_on='protospacer',\n",
    "    how='left'\n",
    ")\n",
    "print(ref_clean_sub_poolf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the intended_target_chr/intended_target_start/intended_target_end values, which are given as 'chr_element', 'chr_start_element', 'chr_end_element'\n",
    "# Note that this refers to the element being targeted, not the gene itself\n",
    "def add_element_coords(ref_clean_sub, sgrna_index_merged):\n",
    "    ref_clean_sub = pd.merge(\n",
    "        ref_clean_sub,\n",
    "        sgrna_index_merged[['protospacer_upper', 'chr_element', 'chr_start_element', 'chr_end_element']],\n",
    "        left_on='spacer',\n",
    "        right_on='protospacer_upper',\n",
    "        how='left'\n",
    "    )\n",
    "    # Remove protospacer_upper column\n",
    "    ref_clean_sub = ref_clean_sub.drop(columns=['protospacer_upper'])\n",
    "    # Rename intended target names\n",
    "    ref_clean_sub.rename(columns={'chr_element': 'intended_target_chr', \n",
    "                                  'chr_start_element': 'intended_target_start',\n",
    "                                  'chr_end_element': 'intended_target_end'},\n",
    "                                  inplace=True)\n",
    "    print(ref_clean_sub.head())\n",
    "    return ref_clean_sub\n",
    "\n",
    "\n",
    "ref_clean_sub_poolabcd = add_element_coords(ref_clean_sub_poolabcd, sgrna_index_merged)\n",
    "#print(ref_clean_sub_poolabcd[ref_clean_sub_poolabcd['spacer'] == 'CGGCGACCCTAGGAGAGGT'])\n",
    "\n",
    "ref_clean_sub_poolabcd.head()\n",
    "\n",
    "# Columns are already correctly labeled for pool F\n",
    "ref_clean_sub_poolf = pd.merge(\n",
    "    ref_clean_sub_poolf,\n",
    "    sgrna_index_poolf[['protospacer', 'intended_target_chr', 'intended_target_start', 'intended_target_end']],\n",
    "    left_on='spacer',\n",
    "    right_on='protospacer',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_crispr_file.head())\n",
    "\n",
    "# Reorganize columns to match\n",
    "new_order = ['guide_id', 'spacer', 'targeting', 'type', 'guide_chr', 'guide_start', 'guide_end', 'strand', 'pam', 'intended_target_name', 'intended_target_chr', 'intended_target_start', 'intended_target_end', 'label', 'genomic_element']\n",
    "ref_clean_sub_poolabcd = ref_clean_sub_poolabcd[new_order].drop_duplicates()\n",
    "print(ref_clean_sub_poolabcd.head())\n",
    "ref_clean_sub_poolf = ref_clean_sub_poolf[new_order].drop_duplicates()\n",
    "print(ref_clean_sub_poolf.head())\n",
    "\n",
    "controls_in_ref = ref_clean_sub_poolabcd[\n",
    "    ref_clean_sub_poolabcd['spacer'].isin(non_targeting['Photospacer (same for all 3 sets)'])\n",
    "    | ref_clean_sub_poolabcd['spacer'].isin(pos_controls['Photospacer (represent 10 times)'])\n",
    "    | ref_clean_sub_poolabcd['spacer'].isin(neg_spacers)\n",
    "]\n",
    "print(len(controls_in_ref))\n",
    "print(ref_clean_sub_poolabcd['label'].value_counts(dropna=False))\n",
    "#controls_in_ref.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with non-standard chromosomes (i.e. chr1, chr2, chrX, etc., not chrU) by making sure chr is not followed by a letter other than X or Y\n",
    "control_types = ['non_targeting', 'negative_control', 'positive_control']\n",
    "\n",
    "# Mask for control and targeting guides\n",
    "is_control = ref_clean_sub_poolabcd['label'].str.lower().isin(control_types)\n",
    "is_targeting = ~is_control\n",
    "\n",
    "# Standard chromosome pattern\n",
    "standard_chr_pattern = r'^chr(\\d+|X|Y)$'\n",
    "\n",
    "# Split, filter targeting only\n",
    "controls_df = ref_clean_sub_poolabcd[is_control]\n",
    "targets_df = ref_clean_sub_poolabcd[is_targeting]\n",
    "\n",
    "filtered_targets_df = targets_df[\n",
    "    targets_df['guide_chr'].notna()\n",
    "    & targets_df['intended_target_chr'].notna()\n",
    "    & targets_df['guide_chr'].str.match(standard_chr_pattern)\n",
    "    & targets_df['intended_target_chr'].str.match(standard_chr_pattern)\n",
    "]\n",
    "\n",
    "# Recombine targets + controls\n",
    "ref_clean_sub_poolabcd = pd.concat([filtered_targets_df, controls_df], ignore_index=True)\n",
    "ref_clean_sub_poolf = pd.concat([ref_clean_sub_poolf, controls_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are certain examples where a target has the same protospacer sequence but multiple local_target_start and local_target_end\n",
    "# values, which throws errors with the pipeline. This takes the min/max of those values (dependent on strand) and collapses into a single row\n",
    "\n",
    "# Collapse groups of targeting guides that have identical metadata but\n",
    "# multiple start/end coordinates. For negative‑strand entries, we take the\n",
    "# max(start) / min(end); for positive‑strand or mixed, min(start) / max(end).\n",
    "# Control or non‑targeting rows are passed through unchanged.\n",
    "def collapse_grouped_targets(df):\n",
    "\n",
    "    def collapse_group(subdf):\n",
    "        # Handle both guide_ and intended_target_ coordinates\n",
    "        if (subdf[\"strand\"] == \"-\").all():\n",
    "            g_start = subdf[\"guide_start\"].max()\n",
    "            g_end   = subdf[\"guide_end\"].min()\n",
    "            t_start = subdf[\"intended_target_start\"].max()\n",
    "            t_end   = subdf[\"intended_target_end\"].min()\n",
    "        else:\n",
    "            g_start = subdf[\"guide_start\"].min()\n",
    "            g_end   = subdf[\"guide_end\"].max()\n",
    "            t_start = subdf[\"intended_target_start\"].min()\n",
    "            t_end   = subdf[\"intended_target_end\"].max()\n",
    "\n",
    "        row = subdf.iloc[0].copy()\n",
    "        row[\"guide_start\"] = g_start\n",
    "        row[\"guide_end\"] = g_end\n",
    "        row[\"intended_target_start\"] = t_start\n",
    "        row[\"intended_target_end\"] = t_end\n",
    "        return row\n",
    "\n",
    "    # Identify control / non-targeting rows (pass through unchanged)\n",
    "    is_control = (\n",
    "        df[\"type\"].str.contains(\"non\", case=False, na=False)\n",
    "        #| df[\"type\"].str.contains(\"control\", case=False, na=False)\n",
    "    )\n",
    "\n",
    "    controls_df = df[is_control].copy()\n",
    "    targets_df  = df[~is_control].copy()\n",
    "\n",
    "    # Exclude all coordinate columns from grouping\n",
    "    coord_cols = [\n",
    "        \"guide_start\", \"guide_end\",\n",
    "        \"intended_target_start\", \"intended_target_end\"\n",
    "    ]\n",
    "    group_cols = [c for c in targets_df.columns if c not in coord_cols]\n",
    "\n",
    "    collapsed_targets = (\n",
    "        targets_df\n",
    "        .groupby(group_cols, dropna=False)\n",
    "        .apply(collapse_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    combined = pd.concat([collapsed_targets, controls_df], ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "ref_clean_sub_poolabcd = collapse_grouped_targets(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf    = collapse_grouped_targets(ref_clean_sub_poolf)\n",
    "print(ref_clean_sub_poolabcd.shape)\n",
    "print(ref_clean_sub_poolf.shape)\n",
    "print(ref_clean_sub_poolabcd['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also write a version without mostly NA values, duplicate rows\n",
    "#print(ref_clean_sub_poolabcd.shape)\n",
    "#ref_clean_sub_poolabcd_clean = ref_clean_sub_poolabcd.dropna(thresh = (len(ref_clean_sub_poolabcd.columns)/2)).drop_duplicates()\n",
    "#print(ref_clean_sub_poolabcd_clean.shape)\n",
    "#ref_clean_sub_poolabcd_clean.to_csv(local_path + \"harmonized_guide_file_poolabcd_nomissing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrogate duplicate spacers\n",
    "duplicate_spacers_poolf = ref_clean_sub_poolf[ref_clean_sub_poolf[\"spacer\"].duplicated(keep=False)]\n",
    "print(duplicate_spacers_poolf[\"spacer\"].value_counts())\n",
    "\n",
    "duplicate_spacers_poolf_diff = duplicate_spacers_poolf.loc[:, duplicate_spacers_poolf.nunique() > 1]\n",
    "print(duplicate_spacers_poolf_diff.head())\n",
    "print(duplicate_spacers_poolf_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop alternate contigs if a canonical chr exists (e.g. chr22_KI270731v1_random)\n",
    "import re\n",
    "def remove_random_contigs(df):\n",
    "    df = df.copy()\n",
    "    keep_rows = []\n",
    "\n",
    "    canonical_pattern = re.compile(r\"^chr(\\d+|X|Y)$\", re.IGNORECASE)\n",
    "\n",
    "    for spacer, subdf in df.groupby(\"spacer\", group_keys=False):\n",
    "        has_main = (\n",
    "            subdf[\"guide_chr\"].astype(str).str.match(canonical_pattern).any() or\n",
    "            subdf[\"intended_target_chr\"].astype(str).str.match(canonical_pattern).any()\n",
    "        )\n",
    "\n",
    "        if has_main:\n",
    "            mask = (\n",
    "                subdf[\"guide_chr\"].astype(str).str.match(canonical_pattern)\n",
    "                & subdf[\"intended_target_chr\"].astype(str).str.match(canonical_pattern)\n",
    "            )\n",
    "            keep_rows.append(subdf[mask])\n",
    "        else:\n",
    "            # If no canonical version exists, keep all\n",
    "            keep_rows.append(subdf)\n",
    "\n",
    "    cleaned = pd.concat(keep_rows, ignore_index=True)\n",
    "    return cleaned\n",
    "\n",
    "before = len(ref_clean_sub_poolf)\n",
    "ref_clean_sub_poolf = remove_random_contigs(ref_clean_sub_poolf)\n",
    "after = len(ref_clean_sub_poolf)\n",
    "\n",
    "print(f\"Removed {before - after} '_random' contig rows covered by canonical entries.\")\n",
    "\n",
    "duplicate_spacers_poolf = ref_clean_sub_poolf[ref_clean_sub_poolf[\"spacer\"].duplicated(keep=False)]\n",
    "print(duplicate_spacers_poolf[\"spacer\"].value_counts())\n",
    "print(duplicate_spacers_poolf.head(10))\n",
    "\n",
    "duplicate_spacers_poolf_diff = duplicate_spacers_poolf.loc[:, duplicate_spacers_poolf.nunique() > 1]\n",
    "print(duplicate_spacers_poolf_diff)\n",
    "print(duplicate_spacers_poolf_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually remove 11 strand mismatches in Pool F to select the most canonical one\n",
    "canonical_strand_choices = {\n",
    "    \"AGTGAGGACTAACGGGGCA\": \"+\",  # NCF1B.6\n",
    "    \"CAACTTGCCACTCAAACGC\": \"+\",  # GATSL2.3\n",
    "    \"CACGCCAGACCACGACGGA\": \"-\",  # STAG3L2.3\n",
    "    \"CACGTAACGGGACCACACA\": \"+\",  # LOC100101148.2\n",
    "    \"CACTTGCAGGGGCGCGAGG\": \"+\", # LOC541473.2\n",
    "    \"GACGCCCCCGGCCAGGTGA\": \"+\",  # LOC100101148.6\n",
    "    \"GCCGGAGCTACCGGCAGCC\": \"-\",  # GTF2IP1.2\n",
    "    \"GCTCCACCCTTTCCGGGCG\": \"-\",  # STAG3L3.5\n",
    "    \"GCTCCGCCGCTCGGCCCCT\": \"-\",  # GTF2IP1.9\n",
    "    \"GGAAACCGCCAGACACCAA\": \"-\",  # STAG3L2.2\n",
    "    \"GTCCTTCCCGTCGCCTGCA\": \"+\",  # NCF1B\n",
    "}\n",
    "mask_keep = pd.Series(True, index=ref_clean_sub_poolf.index)\n",
    "\n",
    "for spacer, strand in canonical_strand_choices.items():\n",
    "    # Identify all rows for this spacer\n",
    "    idx_all = ref_clean_sub_poolf.index[ref_clean_sub_poolf[\"spacer\"] == spacer]\n",
    "    # Identify those on non-canonical strands\n",
    "    idx_wrong = ref_clean_sub_poolf.index[\n",
    "        (ref_clean_sub_poolf[\"spacer\"] == spacer)\n",
    "        & (ref_clean_sub_poolf[\"strand\"] != strand)\n",
    "    ]\n",
    "    # Mark wrong-strand rows for removal\n",
    "    mask_keep.loc[idx_wrong] = False\n",
    "    print(f\"Keeping {spacer} ({strand}), removing {len(idx_wrong)} opposite-strand rows\")\n",
    "\n",
    "# Apply mask\n",
    "before = len(ref_clean_sub_poolf)\n",
    "ref_clean_sub_poolf = ref_clean_sub_poolf.loc[mask_keep].reset_index(drop=True)\n",
    "after = len(ref_clean_sub_poolf)\n",
    "\n",
    "print(f\"Removed {before - after} strand-mismatched rows.\")\n",
    "print(f\"{ref_clean_sub_poolf['spacer'].duplicated().sum()} duplicate spacers remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the guide IDs \n",
    "# e.g. ARID1A_+_27022504.23-P1P2-1 --> ARID1A#chr1:26696017-26696035(-)\n",
    "# new format: <target_name>#<guide_chr>:<guide_start>-<guide_end>(<strand>)\n",
    "# Only for targeting guides\n",
    "def reformat_guide_ids(df):\n",
    "    df = df.copy()\n",
    "    required_cols = [\"intended_target_name\", \"guide_chr\", \"guide_start\", \"guide_end\", \"strand\"]\n",
    "\n",
    "    mask = (\n",
    "        ~df[\"guide_id\"].str.contains(\"non-targeting\", case=False, na=False)\n",
    "        & df[required_cols].notna().all(axis=1)\n",
    "    )\n",
    "\n",
    "    # Build new guide_id strings only for masked rows\n",
    "    df[\"guide_start\"] = df[\"guide_start\"].astype(\"Int64\")\n",
    "    df[\"guide_end\"] = df[\"guide_end\"].astype(\"Int64\")\n",
    "    df.loc[mask, \"guide_id\"] = (\n",
    "        df.loc[mask, \"intended_target_name\"].astype(str)\n",
    "        + \"#\" + df.loc[mask, \"guide_chr\"].astype(str)\n",
    "        + \":\" + df.loc[mask, \"guide_start\"].astype(str)\n",
    "        + \"-\" + df.loc[mask, \"guide_end\"].astype(str)\n",
    "        + \"(\" + df.loc[mask, \"strand\"].astype(str) + \")\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "ref_clean_sub_poolabcd = reformat_guide_ids(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = reformat_guide_ids(ref_clean_sub_poolf)\n",
    "\n",
    "print(ref_clean_sub_poolabcd.head())\n",
    "print(ref_clean_sub_poolabcd['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For positive controls, replace '_' with '#' in guide ID\n",
    "mask = ref_clean_sub_poolabcd[\"label\"] == \"positive_control\"\n",
    "ref_clean_sub_poolabcd.loc[mask, \"guide_id\"] = ref_clean_sub_poolabcd.loc[mask, \"guide_id\"].astype(str).str.replace(\"_\", \"#\", regex=False)\n",
    "\n",
    "mask = ref_clean_sub_poolf[\"label\"] == \"positive_control\"\n",
    "ref_clean_sub_poolf.loc[mask, \"guide_id\"] = ref_clean_sub_poolf.loc[mask, \"guide_id\"].astype(str).str.replace(\"_\", \"#\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement with coordinates from benchmarking annotation file for controls missing data\n",
    "benchmark_annot = pd.read_csv(local_path + \"benchmark_guide_metadata_v1 - benchmark_guide_metadata_v1.csv\")\n",
    "print(benchmark_annot.head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_controls_from_benchmark(main_df, benchmark_df):\n",
    "    df = main_df.copy()\n",
    "    bench = benchmark_df.copy()\n",
    "\n",
    "    fill_cols = [\"guide_chr\", \"guide_start\", \"guide_end\", \"strand\", \"intended_target_chr\", \"intended_target_start\", \"intended_target_end\"]\n",
    "\n",
    "    # Standardize spacers\n",
    "    df[\"spacer_norm\"] = df[\"spacer\"].str.strip().str.upper()\n",
    "    bench[\"spacer_norm\"] = bench[\"spacer\"].str.strip().str.upper()\n",
    "\n",
    "    # Merge benchmark data\n",
    "    merged = pd.merge(\n",
    "        df,\n",
    "        bench[[\"spacer_norm\"] + fill_cols],\n",
    "        on=\"spacer_norm\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_bench\"),\n",
    "    )\n",
    "\n",
    "    print(\"Matched rows with benchmark:\", merged[\"guide_chr_bench\"].notna().sum())\n",
    "\n",
    "    # Fill only for controls\n",
    "    is_control = merged[\"label\"].isin([\"positive_control\", \"negative_control\"])\n",
    "\n",
    "    for col in fill_cols:\n",
    "        benchcol = f\"{col}_bench\"\n",
    "        # Only fill where original is NaN and benchmark has a real value\n",
    "        merged.loc[is_control & merged[col].isna() & merged[benchcol].notna(), col] = \\\n",
    "            merged.loc[is_control & merged[col].isna() & merged[benchcol].notna(), benchcol]\n",
    "\n",
    "    # Clean up\n",
    "    merged.drop(columns=[f\"{c}_bench\" for c in fill_cols] + [\"spacer_norm\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "controls_with_nans = ref_clean_sub_poolabcd[\n",
    "    ref_clean_sub_poolabcd[\"label\"].isin([\"positive_control\", \"negative_control\"])\n",
    "    & (\n",
    "        ref_clean_sub_poolabcd[\"guide_chr\"].isna()\n",
    "        | ref_clean_sub_poolabcd[\"guide_start\"].isna()\n",
    "        | ref_clean_sub_poolabcd[\"guide_end\"].isna()\n",
    "    )\n",
    "]\n",
    "print(\"Controls missing before:\", controls_with_nans.shape[0])\n",
    "ref_clean_sub_poolabcd = fill_controls_from_benchmark(ref_clean_sub_poolabcd, benchmark_annot)\n",
    "\n",
    "controls_with_nans = ref_clean_sub_poolabcd[\n",
    "    ref_clean_sub_poolabcd[\"label\"].isin([\"positive_control\", \"negative_control\"]) &\n",
    "    (\n",
    "        ref_clean_sub_poolabcd[\"guide_chr\"].isna() |\n",
    "        ref_clean_sub_poolabcd[\"guide_start\"].isna() |\n",
    "        ref_clean_sub_poolabcd[\"guide_end\"].isna()\n",
    "    )\n",
    "]\n",
    "print(\"Controls missing after :\", controls_with_nans.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a merged Pool ABCD and Pool F file\n",
    "base = ref_clean_sub_poolabcd.copy()\n",
    "targeting_from_f = ref_clean_sub_poolf[ref_clean_sub_poolf[\"label\"] == \"targeting\"]\n",
    "\n",
    "ref_clean_sub_poolabcdf = pd.concat([base, targeting_from_f], ignore_index=True)\n",
    "ref_clean_sub_poolabcdf = ref_clean_sub_poolabcdf.drop_duplicates(subset=[\"spacer\"], keep=\"first\")\n",
    "\n",
    "print(ref_clean_sub_poolabcdf.head())\n",
    "print(ref_clean_sub_poolabcdf.shape)\n",
    "print(ref_clean_sub_poolabcdf['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another check for duplicates in the concatenated file\n",
    "duplicate_spacers = ref_clean_sub_poolabcdf[ref_clean_sub_poolabcdf.duplicated(subset=['spacer'])]\n",
    "#print(duplicate_spacers.head())\n",
    "#print(duplicate_spacers.shape)\n",
    "\n",
    "# Find the columns that are different between the duplicate spacers\n",
    "duplicate_spacers_diff = duplicate_spacers.loc[:, duplicate_spacers.nunique() > 1]\n",
    "#print(duplicate_spacers_diff.head())\n",
    "#print(duplicate_spacers_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any Excel-style gene names converted to dates\n",
    "import re\n",
    "month_gene_map = {\n",
    "    \"JAN\": \"JAN\", \"FEB\": \"FEB\", \"MAR\": \"MARCH\", \"APR\": \"APR\",\n",
    "    \"MAY\": \"MAY\", \"JUN\": \"JUN\", \"JUL\": \"JUL\", \"AUG\": \"AUG\",\n",
    "    \"SEP\": \"SEPT\", \"OCT\": \"OCT\", \"NOV\": \"NOV\", \"DEC\": \"DEC\"\n",
    "}\n",
    "\n",
    "def fix_excel_date_genes(symbol):\n",
    "    if not isinstance(symbol, str):\n",
    "        return symbol  # leave NaN or other types alone\n",
    "\n",
    "    m = re.match(r\"^(\\d{1,2})-([A-Za-z]{3})$\", symbol.strip())\n",
    "    if m:\n",
    "        num, month = m.groups()\n",
    "        month = month.upper()\n",
    "        if month in month_gene_map:\n",
    "            return f\"{month_gene_map[month]}{num}\"\n",
    "    return symbol\n",
    "\n",
    "ref_clean_sub_poolabcdf['intended_target_name'] = ref_clean_sub_poolabcdf['intended_target_name'].apply(fix_excel_date_genes)\n",
    "ref_clean_sub_poolabcd['intended_target_name'] = ref_clean_sub_poolabcd['intended_target_name'].apply(fix_excel_date_genes)\n",
    "ref_clean_sub_poolf['intended_target_name'] = ref_clean_sub_poolf['intended_target_name'].apply(fix_excel_date_genes)\n",
    "\n",
    "print(ref_clean_sub_poolabcd['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort to put controls at the top of the file\n",
    "control_order = [\"positive_control\", \"negative_control\", \"non_targeting\"]\n",
    "ref_clean_sub_poolabcdf[\"label\"] = pd.Categorical(ref_clean_sub_poolabcdf[\"label\"], categories=control_order + [\"targeting\"], ordered=True)\n",
    "ref_clean_sub_poolabcd[\"label\"] = pd.Categorical(ref_clean_sub_poolabcd[\"label\"], categories=control_order + [\"targeting\"], ordered=True)\n",
    "ref_clean_sub_poolf[\"label\"] = pd.Categorical(ref_clean_sub_poolf[\"label\"], categories=control_order + [\"targeting\"], ordered=True)\n",
    "\n",
    "ref_clean_sub_poolabcdf = ref_clean_sub_poolabcdf.sort_values(by=[\"label\", \"guide_id\"], ascending=[True, True]).reset_index(drop=True)\n",
    "ref_clean_sub_poolabcd = ref_clean_sub_poolabcd.sort_values(by=[\"label\", \"guide_id\"], ascending=[True, True]).reset_index(drop=True)\n",
    "ref_clean_sub_poolf = ref_clean_sub_poolf.sort_values(by=[\"label\", \"guide_id\"], ascending=[True, True]).reset_index(drop=True)\n",
    "print(ref_clean_sub_poolabcd['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add empty columns for putative_target_genes, reporter, and imperfect\n",
    "def add_placeholder_cols(ref_clean_sub):\n",
    "    ref_clean_sub['putative_target_genes'] = np.nan\n",
    "    ref_clean_sub['reporter'] = np.nan\n",
    "    ref_clean_sub['imperfect'] = np.nan\n",
    "    return ref_clean_sub\n",
    "\n",
    "ref_clean_sub_poolabcdf = add_placeholder_cols(ref_clean_sub_poolabcdf)\n",
    "ref_clean_sub_poolabcd = add_placeholder_cols(ref_clean_sub_poolabcd)\n",
    "ref_clean_sub_poolf = add_placeholder_cols(ref_clean_sub_poolf)\n",
    "print(ref_clean_sub_poolabcdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "ref_clean_sub_poolabcd.to_csv(local_path + \"harmonized_guide_file_poolabcd.csv\", index=False)\n",
    "ref_clean_sub_poolabcd.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolabcd.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")\n",
    "ref_clean_sub_poolf.to_csv(local_path + \"harmonized_guide_file_poolf.csv\", index=False)\n",
    "ref_clean_sub_poolf.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolf.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_clean_sub_poolabcdf.to_csv(local_path + \"harmonized_guide_file_poolabcdf.csv\", index=False)\n",
    "\n",
    "# Write to tsv file, including header\n",
    "ref_clean_sub_poolabcdf.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolabcdf.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pybiomart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert intended_target_name to Ensembl ID using pyBiomart\n",
    "from pybiomart import Dataset\n",
    "\n",
    "dataset = Dataset(name='hsapiens_gene_ensembl', host='http://www.ensembl.org')\n",
    "\n",
    "# Fetch mapping\n",
    "mapping = dataset.query(attributes=['hgnc_symbol', 'ensembl_gene_id', 'external_synonym'])\n",
    "mapping.columns = ['intended_target_name', 'ensembl_gene_id', 'external_synonym']\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine HGNC symbol and synonyms into a single mapping dataframe\n",
    "# Melt external_synonym if it's a comma-separated list\n",
    "mapping_expanded = mapping.copy()\n",
    "mapping_expanded['external_synonym'] = mapping_expanded['external_synonym'].fillna('')\n",
    "mapping_expanded = mapping_expanded.assign(\n",
    "    synonym_list=mapping_expanded['external_synonym'].str.split(',')\n",
    ").explode('synonym_list')\n",
    "mapping_expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine intended_target_name and synonym_list into one lookup table\n",
    "lookup = pd.concat([\n",
    "    mapping_expanded[['intended_target_name', 'ensembl_gene_id']].rename(columns={'intended_target_name': 'symbol'}),\n",
    "    mapping_expanded[['synonym_list', 'ensembl_gene_id']].rename(columns={'synonym_list': 'symbol'})\n",
    "]).drop_duplicates()\n",
    "#print(lookup.head())\n",
    "lookup['symbol'] = lookup['symbol'].apply(lambda x: str(x).upper().replace('-', '').replace('_',''))\n",
    "lookup = lookup.drop_duplicates(subset=[\"symbol\"], keep=\"first\")\n",
    "print(lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_symbol(s):\n",
    "    s = str(s).upper()\n",
    "    # Remove dashes for relaxed matching\n",
    "    s = s.replace('-', '').replace('_', '')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with data frame and replace intended_target_name with Ensembl IDs\n",
    "def replace_w_ensembl(ref_clean, mapping):\n",
    "    # Make all symbols uppercase for matching\n",
    "    ref_clean = ref_clean.copy()\n",
    "    ref_clean['intended_target_name'] = ref_clean['intended_target_name'].apply(clean_symbol)\n",
    "\n",
    "    mapping = mapping.copy()\n",
    "    mapping['symbol'] = mapping['symbol'].apply(clean_symbol)\n",
    "\n",
    "    # Merge by symbol\n",
    "    ref_clean = ref_clean.merge(mapping, left_on='intended_target_name',\n",
    "                                right_on='symbol', how='left')\n",
    "\n",
    "    # Identify missing mappings\n",
    "    missing_mask = ref_clean['ensembl_gene_id'].isna()\n",
    "    num_missing = missing_mask.sum()\n",
    "    missing_genes = ref_clean.loc[missing_mask, 'intended_target_name'].unique()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Number of rows with missing Ensembl mapping: {num_missing}\")\n",
    "    print(f\"Gene symbols with no mapping:\\n{missing_genes}\")\n",
    "\n",
    "    # Replace intended_target_name with Ensembl ID where available,\n",
    "    # otherwise keep the original gene name\n",
    "    ref_clean['intended_target_name'] = ref_clean.apply(\n",
    "        lambda row: row['ensembl_gene_id'] if pd.notna(row['ensembl_gene_id']) \n",
    "                    else row['intended_target_name'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop temp columns if you don’t need them later\n",
    "    ref_clean.drop(columns=['ensembl_gene_id', 'symbol'], inplace=True, errors='ignore')\n",
    "\n",
    "    return ref_clean\n",
    "\n",
    "ref_clean_sub_poolabcdf_ensembl = replace_w_ensembl(ref_clean_sub_poolabcdf, lookup)\n",
    "ref_clean_sub_poolabcd_ensembl = replace_w_ensembl(ref_clean_sub_poolabcd, lookup)\n",
    "ref_clean_sub_poolf_ensembl = replace_w_ensembl(ref_clean_sub_poolf, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "ref_clean_sub_poolabcd_ensembl.to_csv(local_path + \"harmonized_guide_file_poolabcd_ensg.csv\", index=False)\n",
    "ref_clean_sub_poolabcd_ensembl.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolabcd_ensg.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")\n",
    "ref_clean_sub_poolf_ensembl.to_csv(local_path + \"harmonized_guide_file_poolf_ensg.csv\", index=False)\n",
    "ref_clean_sub_poolf_ensembl.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolf_ensg.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")\n",
    "ref_clean_sub_poolabcdf_ensembl.to_csv(local_path + \"harmonized_guide_file_poolabcdf_ensg.csv\", index=False)\n",
    "\n",
    "# Write to tsv file, including header\n",
    "ref_clean_sub_poolabcdf_ensembl.to_csv(\n",
    "    local_path + \"harmonized_guide_file_poolabcdf_ensg.tsv\",\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    lineterminator='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick unit tests to make sure everything is kosher\n",
    "def run_integrity_checks(df, pool_label=\"\"):\n",
    "    print(f\"\\nRunning integrity checks for {pool_label}\")\n",
    "\n",
    "    # Check for duplicate spacers\n",
    "    duplicates = df[df[\"spacer\"].duplicated()]\n",
    "    assert len(duplicates) == 0, f\"{len(duplicates)} duplicate spacers found in {pool_label}\"\n",
    "\n",
    "    # Check NA values are np.nan (not 'NA', 'None', or empty strings)\n",
    "    bad_na = df.isin([\"NA\", \"None\", \"\"]).any().sum()\n",
    "    assert bad_na == 0, f\"{bad_na} non-numeric NA placeholders found in {pool_label}\"\n",
    "\n",
    "    # Check for strange characters in spacer or guide_id\n",
    "    pattern_ok = re.compile(r\"^[ACGTN]+$\", re.IGNORECASE)\n",
    "    bad_spacers = df[~df[\"spacer\"].astype(str).str.match(pattern_ok)]\n",
    "    assert len(bad_spacers) == 0, f\"Unexpected characters in {len(bad_spacers)} spacers\"\n",
    "\n",
    "    # Confirm control guides are present\n",
    "    control_types = [\"non_targeting\", \"positive_control\", \"negative_control\"]\n",
    "    found_controls = {ct: (df[\"label\"].str.lower() == ct).sum() for ct in control_types}\n",
    "    missing_controls = [ct for ct, count in found_controls.items() if count == 0]\n",
    "    assert not missing_controls, f\"Missing control types: {missing_controls}\"\n",
    "\n",
    "    # Confirm coordinate columns are numeric or np.nan\n",
    "    coord_cols = [\"guide_start\", \"guide_end\", \"intended_target_start\", \"intended_target_end\"]\n",
    "    for col in coord_cols:\n",
    "        if col in df.columns:\n",
    "            bad_coords = df[col].dropna().apply(lambda x: isinstance(x, (int, float)))\n",
    "            assert bad_coords.all(), f\"Non-numeric entries in {col}\"\n",
    "\n",
    "    # Confirm chromosome format (allow chr1–22, chrX/Y, and *_random)\n",
    "    chr_cols = [\"guide_chr\", \"intended_target_chr\"]\n",
    "    chr_pattern = re.compile(r\"^chr(\\d+|X|Y)(_.*_random)?$\", re.IGNORECASE)\n",
    "    for col in chr_cols:\n",
    "        if col in df.columns:\n",
    "            # Convert all entries to string once\n",
    "            chr_values = df[col].astype(str)\n",
    "    \n",
    "            # Pick out the 'chrPC' rows\n",
    "            pc_mask = chr_values.str.upper() == \"CHRPC\"\n",
    "            if pc_mask.any():\n",
    "                print(f\"Note: {pc_mask.sum()} rows in {col} are labeled 'chrPC' (placeholder coordinates).\")\n",
    "    \n",
    "            # Normal validity check, ignoring NaNs and chrPC\n",
    "            bad_mask = (\n",
    "                df[col].notna()\n",
    "                & ~pc_mask\n",
    "                & ~chr_values.str.match(chr_pattern)\n",
    "            )\n",
    "    \n",
    "            if bad_mask.any():\n",
    "                print(f\"\\nInvalid chromosome values found in {col} for {pool_label}:\")\n",
    "                print(df.loc[bad_mask, [col, \"guide_id\"]].head(10))\n",
    "                print(df.loc[bad_mask, col].value_counts().head(20))\n",
    "    \n",
    "            assert not bad_mask.any(), f\"Invalid chromosome names in {col}\"\n",
    "\n",
    "    print(f\"All checks passed for {pool_label}\")\n",
    "\n",
    "run_integrity_checks(ref_clean_sub_poolabcd_ensembl, \"ABCD\")\n",
    "run_integrity_checks(ref_clean_sub_poolf_ensembl, \"F\")\n",
    "run_integrity_checks(ref_clean_sub_poolabcdf_ensembl, \"ABCDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cNMF",
   "language": "python",
   "name": "torch-cnmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
